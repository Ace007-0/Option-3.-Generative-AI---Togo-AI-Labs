{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNENbUGXa7qJo/ZSHYcdLxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ace007-0/Option-3.-Generative-AI---Togo-AI-Labs/blob/main/gpt2_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "# Uninstall default torch to avoid version conflicts\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "# Install CUDA-compatible PyTorch\n",
        "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install required NLP libraries\n",
        "!pip install transformers==4.44.2 datasets evaluate nltk pandas peft==0.13.2 sentence-transformers\n"
      ],
      "metadata": {
        "id": "xA7xfpkgIo7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch, tqdm, re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed, get_linear_schedule_with_warmup\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import files\n",
        "import nltk, evaluate\n",
        "\n",
        "# Download tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Set random seed + device\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "F-6PaW2dJAWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Upload Dataset\n",
        "Prepare a CSV named dialogs.csv with two columns:\n",
        "question → user input\n",
        "answer → bot response\n",
        "\"\"\"\n",
        "\n",
        "print(\"Upload dialogs.csv (CSV dataset)\")\n",
        "uploaded = files.upload()\n",
        "dataset_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded dataset: {dataset_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YaLT5l8MJAV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset Class\n",
        "This class processes the dataset into conversational format for training:\n",
        "\"\"\"\n",
        "class ConversationData(Dataset):\n",
        "    def __init__(self, path, tokenizer, max_samples=1000, min_turns=6):\n",
        "        # Load + clean dataset\n",
        "        df = pd.read_csv(path, sep=',', on_bad_lines='skip')\n",
        "        df[\"question\"] = df[\"question\"].astype(str).str.strip()\n",
        "        df[\"answer\"] = df[\"answer\"].astype(str).str.strip()\n",
        "        df = df.dropna(subset=[\"question\", \"answer\"])\n",
        "        df = df[(df['question'].str.strip() != \"\") & (df['answer'].str.strip() != \"\")]\n",
        "\n",
        "        # Build conversations\n",
        "        convs, conv = [], []\n",
        "        for _, row in df.iterrows():\n",
        "            conv.append((\"Human\", row[\"question\"]))\n",
        "            conv.append((\"Bot\", row[\"answer\"]))\n",
        "            if len(conv) >= min_turns:\n",
        "                formatted = self.format_conversation(conv)\n",
        "                if formatted:\n",
        "                    convs.append(formatted)\n",
        "                conv = []\n",
        "        if conv:\n",
        "            formatted = self.format_conversation(conv)\n",
        "            if formatted:\n",
        "                convs.append(formatted)\n",
        "\n",
        "        self.X = convs[:max_samples]\n",
        "        print(\"Dataset size:\", len(self.X))\n",
        "\n",
        "        # Fallback if dataset too small\n",
        "        if len(self.X) == 0:\n",
        "            print(\"Warning: No conversations meet min_turns criteria.\")\n",
        "            self.X = [\"<start> Human: Hello <turn> Bot: Hi <end>\"] * 10\n",
        "\n",
        "        # Tokenization\n",
        "        enc = tokenizer(self.X, max_length=512, truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.input_ids, self.attn_mask = enc.input_ids, enc.attention_mask\n",
        "\n",
        "    def format_conversation(self, conv):\n",
        "        text = \"<start> \"\n",
        "        for r, msg in conv:\n",
        "            text += f\"{r}: {msg} <turn> \"\n",
        "        return text.strip() + \" <end>\"\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.input_ids[i], self.attn_mask[i]\n"
      ],
      "metadata": {
        "id": "Im7kYI4jJAP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Setup\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": \"<start>\",\n",
        "    \"eos_token\": \"<end>\",\n",
        "    \"additional_special_tokens\": [\"<turn>\"]\n",
        "})\n"
      ],
      "metadata": {
        "id": "cQ3TK11mJAM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Few-Shot Baseline (No Training)\n",
        "Before fine-tuning, test GPT-2 directly with handcrafted examples:\n",
        "\"\"\"\n",
        "vanilla_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "vanilla_tokenizer.pad_token = vanilla_tokenizer.eos_token\n",
        "model_fewshot = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
      ],
      "metadata": {
        "id": "e5JAPXP1JAJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for response generation:\n",
        "def generate_response(model, prompt, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\",\n",
        "                       truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs, max_new_tokens=80, do_sample=True,\n",
        "            temperature=0.9, top_k=50, top_p=0.92,\n",
        "            repetition_penalty=1.5, no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return re.sub(r'[\\d\\s]{5,}', '', decoded).strip()\n"
      ],
      "metadata": {
        "id": "JqNWzN0XJBBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run few-shot chat:\n",
        "fixed_inputs = [\n",
        "    \"Hi!!!!! It's nice to see you again!\",\n",
        "    \"Looking for interesting podcasts\",\n",
        "    \"what do you usually do for fun?\",\n",
        "    \"First time with wifi on a plane and oh god is it glorious\",\n",
        "    \"I've always wanted to go to England.\"\n",
        "]\n",
        "\n",
        "def few_shot_chat():\n",
        "    exemplar = \"\"\"Human: What kind of phone(s) do you guys have?\n",
        "Bot: I have a pixel. It's pretty great. Much better than what I had before.\n",
        "Human: Does it really charge all the way in 15 min?\n",
        "Bot: Pretty fast. I've never timed it, but it's under half an hour.\n",
        "Human: so how have you been?\n",
        "Bot: i've been great. what about you?\"\"\"\n",
        "\n",
        "    history = []\n",
        "    print(\"\\n=== FEW-SHOT CHAT (Baseline GPT-2, no fine-tune) ===\")\n",
        "    for u in fixed_inputs:\n",
        "        print(f\"Input: {u}\")\n",
        "        prompt = exemplar + \"\".join(history) + f\"\\nHuman: {u}\\nBot:\"\n",
        "        out = generate_response(model_fewshot, prompt, vanilla_tokenizer)\n",
        "        resp = out.split(\"Bot:\")[-1].split(\"Human:\")[0].strip() or \"Interesting! Tell me more.\"\n",
        "        history.append(f\"Human: {u}\\nBot: {resp}\\n\")\n",
        "        print(f\"Output: {resp}\")\n",
        "\n",
        "few_shot_chat()\n"
      ],
      "metadata": {
        "id": "C6s4q39PJA-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning Setup (Full vs LoRA)\n",
        "# Full GPT-2\n",
        "model_base = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model_base.resize_token_embeddings(len(tokenizer))\n",
        "model_base = model_base.to(device)\n",
        "\n",
        "# LoRA GPT-2\n",
        "model_lora = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model_lora.resize_token_embeddings(len(tokenizer))\n",
        "lora_cfg = LoraConfig(r=32, lora_alpha=64,\n",
        "                      target_modules=[\"c_attn\", \"c_proj\"],\n",
        "                      lora_dropout=0.05, task_type=\"CAUSAL_LM\")\n",
        "model_lora = get_peft_model(model_lora, lora_cfg).to(device)\n",
        "\n",
        "# Dataset + DataLoader\n",
        "dataset = ConversationData(dataset_path, tokenizer, max_samples=1000, min_turns=6)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "CnZdC0FZJA7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function:\n",
        "def train_model(dataloader, model, optimizer, scheduler, epochs=5, save_name=\"model_ckpt\"):\n",
        "    model.train()\n",
        "    for e in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm.tqdm(dataloader, desc=f\"Epoch {e+1}\")\n",
        "        for X, mask in progress_bar:\n",
        "            X, mask = X.to(device), mask.to(device)\n",
        "            out = model(X, attention_mask=mask, labels=X)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "        print(f\"Epoch {e+1}, avg loss={total_loss/len(dataloader):.4f}\")\n",
        "        torch.save(model.state_dict(), f\"{save_name}_epoch{e+1}.pt\")\n"
      ],
      "metadata": {
        "id": "zCEtTg6bJ1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models:\n",
        "print(\"Training Full GPT-2\")\n",
        "opt_base = torch.optim.AdamW(model_base.parameters(), lr=5e-6)\n",
        "sch_base = get_linear_schedule_with_warmup(opt_base, 10, len(loader)*5)\n",
        "train_model(loader, model_base, opt_base, sch_base, epochs=5, save_name=\"full_gpt2\")\n",
        "\n",
        "print(\"Training LoRA GPT-2\")\n",
        "opt_lora = torch.optim.AdamW(model_lora.parameters(), lr=2e-4)\n",
        "sch_lora = get_linear_schedule_with_warmup(opt_lora, 10, len(loader)*10)\n",
        "train_model(loader, model_lora, opt_lora, sch_lora, epochs=10, save_name=\"lora_gpt2\")\n"
      ],
      "metadata": {
        "id": "IXMu3kR3J1WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuned Chats\n",
        "def tuned_chat(model, title=\"model\"):\n",
        "    history = []\n",
        "    print(f\"\\n=== Chat with {title} ===\")\n",
        "    for u in fixed_inputs:\n",
        "        print(f\"Input: {u}\")\n",
        "        prompt = \"<start> \" + \" <turn> \".join(history) + f\"Human: {u} <turn> Bot:\"\n",
        "        out = generate_response(model, prompt, tokenizer)\n",
        "        resp = out.split(\"Bot:\")[-1].split(\"Human:\")[0].strip() or \"Hmm, can you expand?\"\n",
        "        history.append(f\"Human: {u} <turn> Bot: {resp}\")\n",
        "        print(f\"Output: {resp}\")\n",
        "\n",
        "tuned_chat(model_base, title=\"Full Fine-Tuned GPT-2\")\n",
        "tuned_chat(model_lora, title=\"LoRA Fine-Tuned GPT-2\")\n"
      ],
      "metadata": {
        "id": "bJfPghsjJ1Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (BLEU + Perplexity)\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def evaluate_model(model, test_prompt, reference, tokenizer):\n",
        "    response = generate_response(model, test_prompt, tokenizer) or \"Default response\"\n",
        "    inputs = tokenizer(response, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    ppl = torch.exp(loss).item()\n",
        "    bleu_score = bleu.compute(predictions=[response], references=[[reference]])[\"bleu\"]\n",
        "    return {\"response\": response, \"perplexity\": ppl, \"BLEU\": bleu_score}\n",
        "\n",
        "print(\"\\n=== Evaluation Comparison Table ===\")\n",
        "ref = \"Human: hi, how are you doing? Bot: I'm fine, thanks for asking.\"\n",
        "prompt_few = \"Human: hi, how are you doing? Bot:\"\n",
        "prompt_tuned = \"<start> Human: hi, how are you doing? <turn> Bot:\"\n",
        "\n",
        "# Few-shot\n",
        "few_shot_response = generate_response(model_fewshot, prompt_few, vanilla_tokenizer)\n",
        "inputs_few = vanilla_tokenizer(few_shot_response, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    loss_few = model_fewshot(**inputs_few, labels=inputs_few[\"input_ids\"]).loss\n",
        "few_shot_ppl = torch.exp(loss_few).item()\n",
        "few_shot_bleu = bleu.compute(predictions=[few_shot_response], references=[[ref]])[\"bleu\"]\n",
        "\n",
        "# Fine-tuned models\n",
        "full_eval = evaluate_model(model_base, prompt_tuned, ref, tokenizer)\n",
        "lora_eval = evaluate_model(model_lora, prompt_tuned, ref, tokenizer)\n",
        "\n",
        "results = pd.DataFrame([\n",
        "    {\"Model\": \"Few-Shot GPT-2\", \"Perplexity\": few_shot_ppl, \"BLEU\": few_shot_bleu},\n",
        "    {\"Model\": \"Full Fine-Tuned GPT-2\", \"Perplexity\": full_eval[\"perplexity\"], \"BLEU\": full_eval[\"BLEU\"]},\n",
        "    {\"Model\": \"LoRA Fine-Tuned GPT-2\", \"Perplexity\": lora_eval[\"perplexity\"], \"BLEU\": lora_eval[\"BLEU\"]},\n",
        "])\n",
        "print(results.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "b0EFMALKJ1Qh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}